@InProceedings{CVT,
    author    = {Zhou, Brady and Kr\"ahenb\"uhl, Philipp},
    title     = {Cross-View Transformers for Real-Time Map-View Semantic Segmentation},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {13760-13769}
}

@InProceedings{Center-Point,
    author    = {Yin, Tianwei and Zhou, Xingyi and Krahenbuhl, Philipp},
    title     = {Center-Based 3D Object Detection and Tracking},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {11784-11793}
}

@misc{BEVdet,
      title={BEVDet: High-performance Multi-camera 3D Object Detection in Bird-Eye-View}, 
      author={Junjie Huang and Guan Huang and Zheng Zhu and Yun Ye and Dalong Du},
      year={2022},
      eprint={2112.11790},
      archivePrefix={arXiv},
      primaryClass={id='cs.CV' full_name='Computer Vision and Pattern Recognition' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.'}
}

@inproceedings{M2BEV,
  archiveprefix = {arxiv},
  eprint        = {2204.05088},
  title         = {{M$^2$BEV}: Multi-camera joint 3{D} detection and segmentation with unified birds-eye view representation},
  author        = {Xie, Enze and Yu, Zhiding and Zhou, Daquan and Philion, Jonah and Anandkumar, Anima and Fidler, Sanja and Luo, Ping and Alvarez, Jose M},
  booktitle     = {ArXiv Preprint},
  month         = {April},
  year          = {2022},
}

@InProceedings{FIERY,
    author    = {Hu, Anthony and Murez, Zak and Mohan, Nikhil and Dudas, Sof{\'\i}a and Hawke, Jeffrey and Badrinarayanan, Vijay and Cipolla, Roberto and Kendall, Alex},
    title     = {FIERY: Future Instance Prediction in Bird's-Eye View From Surround Monocular Cameras},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {15273-15282}
}

@INPROCEEDINGS{8814050,
  author={Kim, Youngseok and Kum, Dongsuk},
  booktitle={2019 IEEE Intelligent Vehicles Symposium (IV)}, 
  title={Deep Learning based Vehicle Position and Orientation Estimation via Inverse Perspective Mapping Image}, 
  year={2019},
  volume={},
  number={},
  pages={317-323},
  keywords={},
  doi={10.1109/IVS.2019.8814050}
}

@InProceedings{Monocular-3D-object-detection,
author = {Ma, Xinzhu and Wang, Zhihui and Li, Haojie and Zhang, Pengbo and Ouyang, Wanli and Fan, Xin},
title = {Accurate Monocular 3D Object Detection via Color-Embedded 3D Reconstruction for Autonomous Driving},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}

@InProceedings{MonoLayout,
author = {Mani, Kaustubh and Daga, Swapnil and Garg, Shubhika and Narasimhan, Sai Shankar and Krishna, Madhava and Jatavallabhula, Krishna Murthy},
title = {MonoLayout: Amodal scene layout from a single image},
booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
month = {March},
year = {2020}
}

@ARTICLE{Cross-View-Sensing-Surroundings,
  author={Pan, Bowen and Sun, Jiankai and Leung, Ho Yin Tiga and Andonian, Alex and Zhou, Bolei},
  journal={IEEE Robotics and Automation Letters}, 
  title={Cross-View Semantic Segmentation for Sensing Surroundings}, 
  year={2020},
  volume={5},
  number={3},
  pages={4867-4873},
  keywords={Semantics;Image segmentation;Virtual private networks;Adaptation models;Robot sensing systems;Task analysis;Semantic scene understanding;deep learning for visual perception;visual learning;visual-based navigation;computer vision for other robotic applications},
  doi={10.1109/LRA.2020.3004325}
}

@InProceedings{Argoverse,
author = {Chang, Ming-Fang and Lambert, John and Sangkloy, Patsorn and Singh, Jagjeet and Bak, Slawomir and Hartnett, Andrew and Wang, De and Carr, Peter and Lucey, Simon and Ramanan, Deva and Hays, James},
title = {Argoverse: 3D Tracking and Forecasting With Rich Maps},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@inproceedings{Depth-pred,
 author = {Eigen, David and Puhrsch, Christian and Fergus, Rob},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Depth Map Prediction from a Single Image using a Multi-Scale Deep Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/7bccfde7714a1ebadf06c5f4cea752c1-Paper.pdf},
 volume = {27},
 year = {2014}
}

@InProceedings{Mono-Depth-Estimation,
author = {Fu, Huan and Gong, Mingming and Wang, Chaohui and Batmanghelich, Kayhan and Tao, Dacheng},
title = {Deep Ordinal Regression Network for Monocular Depth Estimation},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}

@INPROCEEDINGS{Transformers,
  author={Tseng, Ching-Yu and Chen, Yi-Rong and Lee, Hsin-Ying and Wu, Tsung-Han and Chen, Wen-Chin and Hsu, Winston H.},
  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={CrossDTR: Cross-view and Depth-guided Transformers for 3D Object Detection}, 
  year={2023},
  volume={},
  number={},
  pages={4850-4857},
  keywords={Measurement;Three-dimensional displays;Pedestrians;Costs;Fuses;Object detection;Detectors},
  doi={10.1109/ICRA48891.2023.10161451}
}


@InProceedings{Model-Scaling-CNN,
  title = 	 {{E}fficient{N}et: Rethinking Model Scaling for Convolutional Neural Networks},
  author =       {Tan, Mingxing and Le, Quoc},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {6105--6114},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/tan19a/tan19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/tan19a.html},
  abstract = 	 {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are given. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves stateof-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet (Huang et al., 2018). Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flower (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.}
}

@InProceedings{ROI-10D,
author = {Manhardt, Fabian and Kehl, Wadim and Gaidon, Adrien},
title = {ROI-10D: Monocular Lifting of 2D Detection to 6D Pose and Metric Shape},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@ARTICLE{VPN,
  author={Pan, Bowen and Sun, Jiankai and Leung, Ho Yin Tiga and Andonian, Alex and Zhou, Bolei},
  journal={IEEE Robotics and Automation Letters}, 
  title={Cross-View Semantic Segmentation for Sensing Surroundings}, 
  year={2020},
  volume={5},
  number={3},
  pages={4867-4873},
  keywords={Semantics;Image segmentation;Virtual private networks;Adaptation models;Robot sensing systems;Task analysis;Semantic scene understanding;deep learning for visual perception;visual learning;visual-based navigation;computer vision for other robotic applications},
  doi={10.1109/LRA.2020.3004325}}

@InProceedings{Geotrans,
author = {Ammar Abbas, Syed and Zisserman, Andrew},
title = {A Geometric Approach to Obtain a Bird's Eye View From an Image},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops},
month = {Oct},
year = {2019}
}

@InProceedings{GeoTransform,
author="Gong, Shi
and Ye, Xiaoqing
and Tan, Xiao
and Wang, Jingdong
and Ding, Errui
and Zhou, Yu
and Bai, Xiang",
editor="Avidan, Shai
and Brostow, Gabriel
and Ciss{\'e}, Moustapha
and Farinella, Giovanni Maria
and Hassner, Tal",
title="GitNet: Geometric Prior-Based Transformation for Birds-Eye-View Segmentation",
booktitle="Computer Vision -- ECCV 2022",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="396--411",
abstract="Birds-eye-view (BEV) semantic segmentation is critical for autonomous driving for its powerful spatial representation ability. It is challenging to estimate the BEV semantic maps from monocular images due to the spatial gap, since it is implicitly required to realize both the perspective-to-BEV transformation and segmentation. We present a novel two-stage Geometry PrIor-based Transformation framework named GitNet, consisting of (i) the geometry-guided pre-alignment and (ii) ray-based transformer. In the first stage, we decouple the BEV segmentation into the perspective image segmentation and geometric prior-based mapping, with explicit supervision by projecting the BEV semantic labels onto the image plane to learn visibility-aware features and learnable geometry to translate into BEV space. Second, the pre-aligned coarse BEV features are further deformed by ray-based transformers to take visibility knowledge into account. GitNet achieves the leading performance on the challenging nuScenes and Argoverse Datasets.",
isbn="978-3-031-19769-7"
}

@article{GeoTrans2,
author = {Jain, Vanita and Wu, Qiming and Grover, Shivam and Sidana, Kshitij and Chaudhary, Gopal and Myint, San Hlaing and Hua, Qiaozhi},
title = {Generating Bird’s Eye View from Egocentric RGB Videos},
journal = {Wireless Communications and Mobile Computing},
volume = {2021},
number = {1},
pages = {7479473},
doi = {https://doi.org/10.1155/2021/7479473},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2021/7479473},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1155/2021/7479473},
abstract = {In this paper, we present a method for generating bird’s eye video from egocentric RGB videos. Working with egocentric views is tricky since such the view is highly warped and prone to occlusions. On the other hand, a bird’s eye view has a consistent scaling in at least the two dimensions it shows. Moreover, most of the state-of-the-art systems for tasks such as path prediction are built for bird’s eye views of the subjects. We present a deep learning-based approach that transfers the egocentric RGB images captured from a dashcam of a car to bird’s eye view. This is a task of view translation, and we perform two experiments. The first one uses an image-to-image translation method, and the other uses a video-to-video translation. We compare the results of our work with homographic transformation, and our SSIM values are better by a margin of 77\% and 14.4\%, and the RMSE errors are lower by 40\% and 14.6\% for image-to-image translation and video-to-video translation, respectively. We also visually show the efficacy and limitations of each method with helpful insights for future research. Compared to previous works that use homography and LIDAR for 3D point clouds, our work is more generalizable and does not require any expensive equipment.},
year = {2021}
}

@InProceedings{BEVsemseg,
    author    = {Peng, Lang and Chen, Zhirong and Fu, Zhangjie and Liang, Pengpeng and Cheng, Erkang},
    title     = {BEVSegFormer: Bird's Eye View Semantic Segmentation From Arbitrary Camera Rigs},
    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    month     = {January},
    year      = {2023},
    pages     = {5935-5943}
}

@INPROCEEDINGS{nuscenes,
  title={nuScenes: A multimodal dataset for autonomous driving},
  author={Holger Caesar and Varun Bankiti and Alex H. Lang and Sourabh Vora and 
          Venice Erin Liong and Qiang Xu and Anush Krishnan and Yu Pan and 
          Giancarlo Baldan and Oscar Beijbom}, 
  booktitle={CVPR},
  year=2020
}

@misc{PointPillars,
      title={PointPillars: Fast Encoders for Object Detection from Point Clouds}, 
      author={Alex H. Lang and Sourabh Vora and Holger Caesar and Lubing Zhou and Jiong Yang and Oscar Beijbom},
      year={2019},
      eprint={1812.05784},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1812.05784}, 
}

@article{BuildingRomeInADay,
author = {Agarwal, Sameer and Furukawa, Yasutaka and Snavely, Noah and Simon, Ian and Curless, Brian and Seitz, Steven M. and Szeliski, Richard},
title = {Building Rome in a day},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/2001269.2001293},
doi = {10.1145/2001269.2001293},
abstract = {We present a system that can reconstruct 3D geometry from large, unorganized collections of photographs such as those found by searching for a given city (e.g., Rome) on Internet photo-sharing sites. Our system is built on a set of new, distributed computer vision algorithms for image matching and 3D reconstruction, designed to maximize parallelism at each stage of the pipeline and to scale gracefully with both the size of the problem and the amount of available computation. Our experimental results demonstrate that it is now possible to reconstruct city-scale image collections with more than a hundred thousand images in less than a day.},
journal = {Commun. ACM},
month = {oct},
pages = {105–112},
numpages = {8}
}

@inproceedings{StructureFromMotion,
author = {Schönberger, Johannes and Frahm, Jan-Michael},
year = {2016},
booktitle = {CVPR},
month = {06},
pages = {},
title = {Structure-from-Motion Revisited},
doi = {10.1109/CVPR.2016.445}
}

@article{10.1145/1141911.1141964,
author = {Snavely, Noah and Seitz, Steven M. and Szeliski, Richard},
title = {Photo tourism: exploring photo collections in 3D},
year = {2006},
issue_date = {July 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {3},
issn = {0730-0301},
url = {https://doi.org/10.1145/1141911.1141964},
doi = {10.1145/1141911.1141964},
abstract = {We present a system for interactively browsing and exploring large unstructured collections of photographs of a scene using a novel 3D interface. Our system consists of an image-based modeling front end that automatically computes the viewpoint of each photograph as well as a sparse 3D model of the scene and image to model correspondences. Our photo explorer uses image-based rendering techniques to smoothly transition between photographs, while also enabling full 3D navigation and exploration of the set of images and world geometry, along with auxiliary information such as overhead maps. Our system also makes it easy to construct photo tours of scenic or historic locations, and to annotate image details, which are automatically transferred to other relevant images. We demonstrate our system on several large personal photo collections as well as images gathered from Internet photo sharing sites.},
journal = {ACM Trans. Graph.},
month = {jul},
pages = {835–846},
numpages = {12},
keywords = {structure from motion, photo browsing, image-based rendering, image-based modeling}
}

@inbook{PhotoTourism,
author = {Snavely, Noah and Seitz, Steven M. and Szeliski, Richard},
title = {Photo tourism: exploring photo collections in 3D},
year = {2023},
isbn = {9798400708978},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3596711.3596766},
abstract = {We present a system for interactively browsing and exploring large unstructured collections of photographs of a scene using a novel 3D interface. Our system consists of an image-based modeling front end that automatically computes the viewpoint of each photograph as well as a sparse 3D model of the scene and image to model correspondences. Our photo explorer uses image-based rendering techniques to smoothly transition between photographs, while also enabling full 3D navigation and exploration of the set of images and world geometry, along with auxiliary information such as overhead maps. Our system also makes it easy to construct photo tours of scenic or historic locations, and to annotate image details, which are automatically transferred to other relevant images. We demonstrate our system on several large personal photo collections as well as images gathered from Internet photo sharing sites.},
booktitle = {Seminal Graphics Papers: Pushing the Boundaries, Volume 2},
articleno = {54},
numpages = {12}
}

@article{LonguetHiggins1981ACA,
  title={A computer algorithm for reconstructing a scene from two projections},
  author={Hugh Christopher Longuet-Higgins},
  journal={Nature},
  year={1981},
  volume={293},
  pages={133-135},
  url={https://api.semanticscholar.org/CorpusID:4327732}
}

@misc{PointPillars,
      title={PointPillars: Fast Encoders for Object Detection from Point Clouds}, 
      author={Alex H. Lang and Sourabh Vora and Holger Caesar and Lubing Zhou and Jiong Yang and Oscar Beijbom},
      year={2019},
      eprint={1812.05784},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1812.05784}, 
}

@misc{PointPainting,
      title={PointPainting: Sequential Fusion for 3D Object Detection}, 
      author={Sourabh Vora and Alex H. Lang and Bassam Helou and Oscar Beijbom},
      year={2020},
      eprint={1911.10150},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1911.10150}, 
}

@inproceedings{CVCNet,
  title={Every View Counts: Cross-View Consistency in 3D Object Detection with Hybrid-Cylindrical-Spherical Voxelization},
  author={Qi Chen and Lin Sun and Ernest C. H. Cheung and Alan Loddon Yuille},
  booktitle={Neural Information Processing Systems},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:227276305}
}

@misc{PMPNet,
      title={LiDAR-based Online 3D Video Object Detection with Graph-based Message Passing and Spatiotemporal Transformer Attention}, 
      author={Junbo Yin and Jianbing Shen and Chenye Guan and Dingfu Zhou and Ruigang Yang},
      year={2020},
      eprint={2004.01389},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2004.01389}, 
}

@misc{SSN,
      title={SSN: Shape Signature Networks for Multi-class Object Detection from Point Clouds}, 
      author={Xinge Zhu and Yuexin Ma and Tai Wang and Yan Xu and Jianping Shi and Dahua Lin},
      year={2020},
      eprint={2004.02774},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2004.02774}, 
}

@misc{CBGS,
      title={Class-balanced Grouping and Sampling for Point Cloud 3D Object Detection}, 
      author={Benjin Zhu and Zhengkai Jiang and Xiangxin Zhou and Zeming Li and Gang Yu},
      year={2019},
      eprint={1908.09492},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1908.09492}, 
}